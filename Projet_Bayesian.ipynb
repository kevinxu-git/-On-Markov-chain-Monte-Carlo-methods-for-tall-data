{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NLSEkp9bgnZn"
   },
   "source": [
    "\n",
    "<h1 align=\"center\">  <font color=\"#b30000\"> <strong> BAYESIAN LEARNING FOR PARTIALLY OBSERVED DYNAMICAL SYSTEMS </strong> </font> </h1>\n",
    "<br>\n",
    "\n",
    "__**Students**__\n",
    " \n",
    " >- ADEIKALAM Pierre\n",
    " >- CHEN Guangyue \n",
    " >- MORALES Katherine\n",
    " >- XU Kevin\n",
    "\n",
    " __**Professors**__\n",
    " >- DOUC Randal\n",
    " >- LECORFF Sylvain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T-FNYxnoEMrN"
   },
   "source": [
    "<h2 align=\"center\">  <font color=\"#0000e6\"> <strong>   ON MARKOV CHAIN MONTE CARLO METHODS FOR TALL DATA </strong> </font> </h2>\n",
    "\n",
    "\n",
    "### Contents\n",
    "\n",
    "[1. Introduction](#s1)<br>\n",
    "[2. Naive Approaches](#s2)<br>\n",
    "[3. Confidence Sampler](#s3)<br>\n",
    "[4. Conclusion](#s4)<br>\n",
    "[5. Code](#s5)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BUtY4OcHEUNw"
   },
   "source": [
    "<a id='s1'></a>\n",
    "## <font color='#0000e6'> 1. Introduction </font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "blQwITv0ExDC"
   },
   "source": [
    "### <font color='#0000e6'>  1.0 The Paper </font>\n",
    "\n",
    ">The research paper this work is based on is \"**On Markov chain Monte Carlo methods for tall data\"** by [ R. Bardenet, A. Doucet and C. Holmes.](http://www.jmlr.org/papers/volume18/15-205/15-205.pdf)\n",
    ">\n",
    ">\n",
    "> This paper aims to tackle the problem of applying Markov chain Monte Carlo (MCMC) methods for Bayesian inference on *tall* datasets where the number of observations $n$ is very large. \n",
    ">\n",
    ">As we will see, the quantity of observations in a dataset plays a crucial role in the speed and effectiveness of MCMC methods relying on the Metropolis-Hastings sampling algorithm, and the authors propose diverse workarounds to try and preserve the target distribution while **alleviating the computational workload** needed to implement such methods.\n",
    ">\n",
    "> The first aim of this notebook is to provide readers an easy to digest overview of some methods they might naïvely try in practice and give compelling arguments based on the authors' work as to why these naïve methods should not be used. Then, we will try to give an overview of a much more promising approach developed by Bardenet et. al based on a technique called confidence sampling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "duTQOre33db8"
   },
   "source": [
    "### <font color='#0000e6'>  1.1 Bayesian Inference </font>\n",
    "\n",
    ">Let $X = \\{x_1, \\dots, x_n \\} \\subset  \\mathbb R ^d$  be a dataset and a parameter space  $\\Theta$. We assume the observations are conditionally independent, with the associated average log-likelihood being given by:\n",
    ">\n",
    ">\\begin{equation}\n",
    "\\tag{1}\n",
    " l(\\theta) = \\frac{1}{n}\\sum_{i=1}^{n} \\log p(x_i|\\theta).\n",
    "\\end{equation}\n",
    ">\n",
    ">**Bayesian approach:**\n",
    ">\n",
    ">Let $p(\\theta)$ be the prior distribution of  $\\theta$, then the posterior distribution of $\\theta$ is given by:\n",
    ">\n",
    ">\\begin{align}\n",
    "\\tag{2}\n",
    " \\pi(\\theta) = p(\\theta | X) = \\frac{p(\\theta) \\prod_{i=1}^{n} p(x_i|\\theta)}{p(X)}\\; \\; \\propto \\; \\;\\gamma(\\theta),\n",
    "\\end{align}\n",
    ">where $\\gamma(\\theta):=  p(\\theta) e^{(n\\; l(\\theta))}$ is the unnormalized version of $\\pi$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mZKaktS27CXQ"
   },
   "source": [
    "### <font color='#0000e6'>  1.2 Metropolis-Hastings Algorithm </font>\n",
    "\n",
    "> To sample from the distribution $\\pi$, one can use the Metropolis-Hastings (MH) algorithm to simulate a Markov chain  $(\\theta_k)_{k \\geq1}$ of invariant distribution $\\pi$\n",
    ">\n",
    ">\n",
    ">---\n",
    "**Algorithm 1:**  Metropolis-Hastings\n",
    ">\n",
    ">---\n",
    ">\n",
    ">**Input:** \n",
    "  - $\\gamma(\\cdot)$: unnormalized version of $\\pi$\n",
    "  - $q(\\cdot|\\cdot)$: proposal distribution\n",
    "  - $N_{iter}$: number of iterations\n",
    "<H6> \n",
    ">\n",
    ">**For** $k\\leftarrow$ 1 : $N_{iter}$\n",
    "  1.  $\\theta \\leftarrow \\theta_{k-1}$\n",
    "  2.  $\\theta' \\sim q( . | \\theta)$\n",
    "  3.  <font color='#0000e6'> $\\alpha(\\theta, \\theta ') \\leftarrow \\frac{\\gamma(\\theta ')}{\\gamma(\\theta)}\\frac{q( \\theta | \\theta')}{q( \\theta' | \\theta)} $ </font>\n",
    "  4. $ U \\sim \\mathcal U(0,1)$\n",
    "  5.  **If** $U < \\alpha(\\theta, \\theta ')$\n",
    "    - $\\theta_k \\leftarrow \\theta '$\n",
    "  6. **else** \n",
    "    - $\\theta_k \\leftarrow \\theta$\n",
    ">\n",
    ">Return: $(\\theta_k)_{k =1, \\dots, N_{iter}}$\n",
    "></H6>\n",
    ">\n",
    ">---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dikbCGSJaxmY"
   },
   "source": [
    ">The observations of the dataset are independent. Thus, following equation (2), we can rewrite step 3 of the above algorithm to \n",
    ">\\begin{equation}\n",
    "\\tag{3}\n",
    " \\log \\alpha(\\theta, \\theta ')  = \\log \\Big [ \\frac{p(\\theta')}{p(\\theta)}\\frac{q(\\theta | \\theta')}{q(\\theta' | \\theta)} \\Big] +n \\big[ l(\\theta ') -l(\\theta) \\big] .\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vzkOlZ7TdShj"
   },
   "source": [
    "**Problem:**\n",
    "\n",
    ">Since $l(\\theta) = \\frac{1}{n}\\sum_{i=1}^{n} \\log p(x_i|\\theta)$, computing $l(\\theta')$ and $l(\\theta)$ requires a full pass over the dataset, which renders this algorithm unapplicable in practice as the computation needed for a single iteration will be too heavy.\n",
    ">\n",
    ">\n",
    "> The authors propose three types of workarounds:\n",
    ">\n",
    ">- **Divide and conquer approaches:** Divide the data into batches, run the MH algorithm on each batch separately and combine the results.\n",
    ">\n",
    ">- **Exact subsampling approaches:** Use unbiased estimators of the target distribution and plug them into the computation of the acceptance ratio of the MH algorithm.\n",
    ">\n",
    ">- **Approximate subsampling approaches:** Instead of approximating the target distribution, directly try to approximate the acceptance ratio such that the resulting stationary distribution is close enough to the target distribution.\n",
    "\n",
    "**Our work:**\n",
    ">In this notebook, we will look at a naïve approach from each type of solution. Sadly, we will show that their most basic form do not provide simple solutions to the application of MCMC methods for  tall datasets. However, they are very helpful to understand several potential approaches to scale up MCMC on tall datasets, and each of them could be tried.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "398GoGrFuzxS"
   },
   "source": [
    "<a id='s2'></a>\n",
    "## <font color='#0000e6'>   2. Naïve Approaches  </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QHrDWfidlrX7"
   },
   "source": [
    "\n",
    "\n",
    "####  <font color='#0000e6'> 2.1 Divide and conquer  </font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dLyhsnSUGkQ3"
   },
   "source": [
    "> Divide-and-conquer approaches appear as a natural way of handling tall data as they divide the initial dataset into batches, run MCMC on each batch in parallel, and then combine these results to obtain an approximation of the posterior distribution. Formally, assuming the data $X$ is divided into batches $x_1, ... , x_B$,  the latter task can be written as: \n",
    "> \n",
    "> \n",
    ">\\begin{equation}\n",
    "\\tag{4}\n",
    "\\pi(\\theta) = p(\\theta|X) \\propto \\prod_{i=1}^{B}p(\\theta)^{\\frac{1}{B}}p(x_i|\\theta) \\propto \\prod_{i=1}^{B} \\pi_i(\\theta)\n",
    "\\end{equation}\n",
    ">\n",
    "> which is justified **only if we assume the batch posteriors $\\pi_i$ to be Gaussian.**\n",
    ">\n",
    "> Two main problems arise: \n",
    "- **MCMC methods do not return posteriors as mathematical functions**, so an extra step is needed to combine the batch posteriors using this trick.\n",
    "- **How to efficiently combine non-Gaussian batch posteriors?**\n",
    "\n",
    "> The authors mention different ways of combining the batch posteriors:\n",
    ">\n",
    ">-  Fit a Gaussian approximation to the MCMC draws of each batch posterior and combine these approximations using $(4)$ [(Huang and Gelman, 2005)](http://www.stat.columbia.edu/~gelman/research/unpublished/comp7.pdf).\n",
    ">\n",
    ">-  Compute a weighted average of the MCMC draws of each batch posterior. Under Gaussian assumptions , this average should follow the same distribution as the combined batch posteriors [(Scott et al., 2013)](https://www.researchgate.net/publication/295098887_Bayes_and_Big_Data_The_Consensus_Monte_Carlo_Algorithm).\n",
    ">\n",
    ">-  For each batch $x_i$, target the artificial posterior $\\pi_i(\\theta) \\propto p(\\theta)^{\\frac{1}{B}}p(x_i|\\theta)$. Then, fit a smooth approximation to each batch posterior $\\pi_i$ and multiply them [(Neiswanger\n",
    "et al., 2014)](http://www.cs.cmu.edu/~epxing/papers/2014/Neiswanger_Wang_Xing_UAI14a.pdf).\n",
    ">\n",
    ">However, these methods are theoretically justified only when batch posteriors are Gaussian, or **when the size of each batch goes to infinity**, which defeats the purpose of this approach.\n",
    ">\n",
    ">**The supports of the $\\pi_i$ can be disjoint and as a result the product of their approximations will yield a poor approximation of $\\pi$.**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I7feYcSJv6ag"
   },
   "source": [
    "\n",
    "\n",
    "####  <font color='#0000e6'> 2.2 Pseudo-Marginal Metropolis-Hastings: The challenge of Unbiased Estimation  </font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rJ9B_JzzwfTW"
   },
   "source": [
    "> Pseudo-Marginal MH consists in using an unbiased *almost surely non-negative* **estimator $\\hat\\gamma(\\theta)$** of the unnormalized target distribution $\\gamma(\\theta)$ instead of evaluating $\\gamma(\\theta)$ at each iteration of the MH algorithm.\n",
    ">\n",
    "> We could then replace the evaluation of $\\gamma(\\theta)$ by an evaluation of $\\hat\\gamma(\\theta)$ which should be faster to compute and still give the correct acceptance decision most of the time.\n",
    ">\n",
    "> We recall that :\n",
    ">\n",
    "> $$ \\gamma(\\theta) = p(\\theta)e^{nl(\\theta)} $$ with $$nl(\\theta) = \\sum_{i=1}^{n} l_i(\\theta)$$\n",
    ">\n",
    ">\n",
    "> The most naïve estimator one could build for $nl(\\theta)$ is:\n",
    ">\n",
    "> $$n\\hat l(\\theta) = \\frac{n}{t} \\sum_{i = 1}^{t}\\mathrm{log} p(x_i^* | \\theta)$$\n",
    ">\n",
    "> which is an unbiased estimator of $nl(\\theta)$.\n",
    ">\n",
    "> However, $\\mathbb{E}[e^{n \\hat l (\\theta)}] \\neq e^{\\mathbb{E}[n\\hat l(\\theta)]}$ which means that $e^{n\\hat l(\\theta)}$ is not an unbiased estimator of $e^{nl(\\theta)}$.\n",
    ">\n",
    "> [Jacob & Thierry (2015)](https://arxiv.org/abs/1309.6473v1)  showed that it is actually *impossible* to build an unbiased estimator of $e^{nl(\\theta)}$ with just an unbiased estimator of $n\\hat l(\\theta)$ without making any further assumptions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PZMcPKauUyPA"
   },
   "source": [
    "> We can build an unbiased estimator of $e^{nl(\\theta)}$ if for example we were to **assume for all $i$ and for all $\\theta$ that there exists $a(\\theta)$ such that $l_i(\\theta) \\gt a(\\theta)$**. Then, we can apply ([Rhee and Glynn, 2015](https://pdfs.semanticscholar.org/8a84/01f86ca70891293d41ea1c3c0d7877f7910d.pdf), Theorem 1) to build an unbiased estimator of $e^{n(l(\\theta) - a(\\theta))}$:\n",
    ">\n",
    "> *The following formulas of the estimators and inequalities are given for the sake of rigor, but the takeaway message of this section relies only on the very last inequality which highlights the inapplicability of this naïve method.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5kyME9LzU-aL"
   },
   "source": [
    "> For $j\\geq 1$, we draw independently with replacement $t$ samples $(x_{1, j}^*, ..., x_{t, j}^*)$ and define\n",
    ">\n",
    "> $$ D_j^* = \\frac{n}{t} \\sum_{i = 1}^{t}\\mathrm{log} p(x_{i,j}^* | \\theta) - na(\\theta) = n \\hat l_j(\\theta) - na(\\theta)$$\n",
    ">\n",
    "> Then, $D_j$ is an unbiased estimator of $n(l(\\theta) - a(\\theta)$. Let $N$ be a geometric random variable with the parameter $\\frac{\\epsilon}{\\epsilon + 1}$, $\\epsilon \\gt 0$. If we take:\n",
    ">\n",
    "> $$ Z = e^{na(\\theta)}\\left[1 + \\sum_{k = 1}^{N} \\frac{P(N \\geq k)}{k!} \\prod_{j = 1}^{k}D_j^* \\right] $$\n",
    ">\n",
    "> Then by Theorem 1 of (Rhee and Glynn, 2015), $Z$ is an unbiased estimator of $e^{n(l(\\theta) - a(\\theta))}$. However, it is crucial if we want to plug $\\hat \\gamma(\\theta) = Z p(\\theta) $ in the MH algorithm that we control the variance of Z.\n",
    ">\n",
    "> From ([Rhee and Glynn, 2015](https://pdfs.semanticscholar.org/8a84/01f86ca70891293d41ea1c3c0d7877f7910d.pdf)), we have the following lower bound on the *relative* variance of Z:\n",
    ">\n",
    "> $$ \\frac{Var Z}{e^{2nl(\\theta)}} \\geq \\frac{e^{-2n(l(\\theta) - a(\\theta)) + 2n\\sqrt{(1 + \\epsilon )\\left[ \\sigma_t(\\theta)^2 + (l(\\theta) - a(\\theta))^2\\right]}}}{n\\sqrt{(1 + \\epsilon)\\left[ \\sigma_t(\\theta)^2 + (l(\\theta) - a(\\theta))^2\\right]}} + \\mathcal{O}(1)$$\n",
    ">\n",
    "> The important bit of this lower bound is given by $$e^{-2n(l(\\theta) - a(\\theta)) + 2n\\sqrt{(1 + \\epsilon )\\left[ \\sigma_t(\\theta)^2 + (l(\\theta) - a(\\theta))^2\\right]}}$$\n",
    ">\n",
    "> with $\\sigma_t(\\theta)$ being the standard deviation of $\\hat l(\\theta)$.\n",
    ">\n",
    "> We can see that for the relative variance to not explode exponentially with $n$, it is necessary for $n\\sigma_t(\\theta)$ to be of order $1$. From the Central Limit Theorem, we know that $\\sigma_t(\\theta)$ is of order $t^{-1/2}$ which means that the subsample size $t$ should be of order $n^2$ which completely defeats the purpose of this approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Dn1_Tqj6wfha"
   },
   "source": [
    "\n",
    "####  <font color='#0000e6'> 2.3 Naive subsampling </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eKfTz7fC1Qgv"
   },
   "source": [
    "> Assuming we gave up on building an unbiased estimator of $e^{nl(\\theta)}$, naïve subsampling is a naïve Pseudo-Marginal MH technique that consists in plugging inside MH the following unbiased estimator of the average loglikelihood:\n",
    "$$\\hat{l}(\\theta)=\\frac 1n \\sum_{i=1}^{n}\\frac{z_i}{\\lambda}l_i(\\theta)$$ \n",
    ">where the $z_i \\sim  B(1, \\lambda)$ are i.i.d. <br>\n",
    ">\n",
    "> We know from the previous section that this estimator does not preserve the target distribution. However, *(Doucet et al., 2015)* have shown that under Gaussian assumptions, Pseudo-Marginal MH can still be effective if the variance of the average loglikelihood estimation is kept to 1 or 3 depending on the performance of the vanilla MH algorithm.\n",
    ">\n",
    "> We have chosen to illustrate the Naïve subsampling approach as the variance of the average loglikelihood can easily be written:\n",
    ">\n",
    ">\\begin{align}\n",
    "Var [\\hat{l}(\\theta)] &= Var \\Big[\\sum_{i=1}^{n} \\frac{z_i}{\\lambda}l_i(\\theta)\\Big] = \\frac 1 {\\lambda^2}\\sum_{i=1}^{n} l_i(\\theta)^2 Var[z_i] \\\\ \n",
    "&=\\frac{(1-\\lambda)}{\\lambda} \\sum_{i=1}^{n} l_i(\\theta)^2\n",
    "\\end{align}\n",
    ">\n",
    "> Since $\\sum_{i=1}^{n} l_i(\\theta)^2$ grows with $n$, for $Var [\\hat{l}(\\theta)]$ to be close to 1 we need $\\lambda$ to be close to 1. \n",
    "> As $\\lambda$ is the parameter of the Bernoulli subsampling distribution then **there is no significant advantage between using this technique  or the complete data.**\n",
    "\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y97vvdrKxec0"
   },
   "source": [
    "<a id='s2'></a>\n",
    "## <font color='#0000e6'>   3. Confidence sampler  </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XYYSJQ_-mthx"
   },
   "source": [
    "\n",
    "####  <font color='#0000e6'> 3.1 Concept  </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cOE9ze4ymzbK"
   },
   "source": [
    "> If we take a step back and look at the Metropolis-Hastings algorithm, we see that if we could just approximate the *acceptance ratio* $\\alpha(\\theta, \\theta ')$ such that we are *confident* enough that our approximation will give the same decision as the actual acceptance ratio, then the stationary target distribution of this chain should be preserved. This is what the *confidence sampling* approach is about.\n",
    ">\n",
    "> We recall that the acceptance ratio is given by:\n",
    "> $$\\alpha(\\theta, \\theta ') = \\min\\left\\{\\frac{\\pi(y_{t+1})q(x_t,y_{t+1})}{\\pi(x_t)q(y_{t+1},x_t)},1\\right\\} $$\n",
    ">\n",
    "> If we let $u \\sim \\mathcal{U}(0, 1)$, then the acceptance decision is given by:\n",
    ">\n",
    "> $$ \\alpha(\\theta, \\theta ') \\geq u \\iff \\mathrm{log}\\alpha(\\theta, \\theta ') \\geq \\mathrm{log}(u)$$\n",
    ">\n",
    "> which is equivalent to:\n",
    ">\n",
    "> $$ \\frac{1}{n}\\sum_{i=1}^{n}\\mathrm{log}\\left(\\frac{p(x_i|\\theta ')}{p(x_i|\\theta)}\\right) \\geq \\frac{1}{n}\\mathrm{log}u - \\frac{1}{n} \\mathrm{log}\\left(\\frac{p(\\theta ')q(\\theta | \\theta')}{p(\\theta)q(\\theta' | \\theta)}\\right)$$\n",
    ">\n",
    "> If we can approximate the loglikelihood ratio on the left well enough, then the acceptance decision taken by using this approximation should be the same than the one taken with the full loglikelihood ratio. Let us now build such a procedure.\n",
    ">\n",
    "> For all $\\theta, \\theta ' \\in \\Theta$, we assume that there exists $C_{\\theta, \\theta '} \\gt 0$ such that:\n",
    ">\n",
    "> $$ \\max_{i = 1,...,n} \\left|\\mathrm{log}\\left( \\frac{p(x_i |\\theta)}{p(x_i | \\theta ')} \\right) \\right| \\leq C_{\\theta, \\theta '}$$\n",
    ">\n",
    ">\n",
    "> Fix $\\theta, \\theta ' \\in \\Theta$. We can use a *Bernstein* or *Hoeffding* type of **concentration inequality** to bound our approximation of the loglikelihood ratio with respect to the number of samples used. We obtain, for a subsample $(x_1^*, ... , x_t^*)$ of size $t$, that :\n",
    ">\n",
    "> $$\\mathbb{P}\\left(\\left|\\frac{1}{t} \\sum_{i=1}^{t} \\mathrm{log}\\left( \\frac{p(x_i^* |\\theta)}{p(x_i^* | \\theta ')}\\right) - \\frac{1}{n} \\sum_{i=1}^{n} \\mathrm{log}\\left( \\frac{p(x_i^* |\\theta)}{p(x_i^* | \\theta ')}\\right) \\right| \\leq c_t(\\delta) \\right) \\geq 1 - \\delta$$\n",
    ">\n",
    "> where $$c_t(\\delta) = C_{\\theta, \\theta '} \\sqrt{\\frac{2(1 - f_t^*)\\mathrm{log}(2/\\delta)}{t}}$$\n",
    ">\n",
    "> and $f_t^* = \\frac{t-1}{n}$ is approximately the fraction of used samples.\n",
    ">\n",
    "> For readers that are unfamiliar with concentration inequalities, this inequality roughly translates into:\n",
    ">\n",
    "> $$\\mathbb{P}\\left(\\left| approximation - real \\hspace{0.1cm}value \\right| \\leq constant \\hspace{0.1cm} depending \\hspace{0.1cm} on\\hspace{0.1cm} \\delta \\right) \\geq 1 - \\delta $$\n",
    "> \n",
    "> Let $$\\psi(u, \\theta, \\theta') = \\frac{1}{n}\\mathrm{log}u - \\frac{1}{n} \\mathrm{log}\\left(\\frac{p(\\theta ')q(\\theta | \\theta')}{p(\\theta)q(\\theta' | \\theta)}\\right)$$\n",
    "> $$\\Lambda_t(\\theta, \\theta')= \\frac{1}{t} \\sum_{i=1}^{t} \\mathrm{log}\\left( \\frac{p(x_i^* |\\theta)}{p(x_i^* | \\theta ')}\\right)$$\n",
    "> \n",
    "> and let us place ourselves on the event $\\left\\{ \\left| \\Lambda_t(\\theta, \\theta') - \\Lambda_n(\\theta, \\theta') \\right| \\leq c_t(\\delta)\\right\\}$, which is the event described by the concentration inequality.\n",
    ">\n",
    "> It follows from simple computation that on this event, if the event $\\left\\{ \\left| \\Lambda_t(\\theta, \\theta') - \\psi(u, \\theta, \\theta') \\right| \\gt c_t(\\delta)\\right\\}$ also holds, the approximated acceptance ratio will yield the correct decision. Indeed, both $\\Lambda_t(\\theta, \\theta')$ and $\\Lambda_n(\\theta, \\theta')$ will be on the same side of $\\psi(u, \\theta, \\theta')$, so the decision will be the same. The idea of confidence sampling is then to sequentially increase $t$ until $\\left\\{ \\left| \\Lambda_t(\\theta, \\theta') - \\psi(u, \\theta, \\theta') \\right| \\gt c_t(\\delta)\\right\\}$ holds (which will eventually be the case as $c_t(\\delta) \\rightarrow 0$ when $t \\rightarrow n$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bVlSuUYzm1Tm"
   },
   "source": [
    "\n",
    "####  <font color='#0000e6'> 3.2 Algorithm : Confidence Sampler MH (Bardenet et al., 2014)  </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Hyz8tulom5Tg"
   },
   "source": [
    "#####  <font color='#0000e6'> 3.2.1 Pseudocode of the algorithm  </font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x2i-QObeiSD7"
   },
   "source": [
    ">---\n",
    "**Algorithm 2:**  Confidence Sampler\n",
    ">\n",
    ">---\n",
    ">\n",
    ">**Input:** \n",
    "  + $p(x|\\theta)$ : A likelihood \n",
    "  + $p(\\theta)$ : A prior distribution\n",
    "  + $q(\\theta'|\\theta)$ : A proposal distribution\n",
    "  + $\\theta_0$ : A starting point\n",
    "  + $N_{iter}$ : Number of iterations\n",
    "  + $\\mathcal{X}$ : The dataset\n",
    "  + $\\delta$ : The probability for the concentration inequality\n",
    "  + $C_{\\theta, \\theta'}$ : The range of each log likelihood ratio\n",
    "  + $\\gamma$ : The ratio at which the batchsize is increased geometrically\n",
    "> \n",
    ">**Output:**\n",
    "  + a Markov chain\n",
    ">\n",
    "> 1. **for** $k \\leftarrow 1$ **to** $N_{iter}$\n",
    "> 2. $\\hspace{1cm} \\theta \\leftarrow \\theta_{k-1}$\n",
    "> 3. $\\hspace{1cm} \\theta' \\sim q(.|\\theta), u \\sim \\mathcal{U}_{(0,1)}$,\n",
    "> 4. $\\hspace{1cm} \\psi(u, \\theta, \\theta') \\leftarrow \\dfrac{1}{n} \\log \\left[ u \\dfrac{p(\\theta)q(\\theta'|\\theta)}{p(\\theta')q(\\theta|\\theta')} \\right]$\n",
    "> 5. $\\hspace{1cm} t \\leftarrow 0$\n",
    "> 6. $\\hspace{1cm} t_{look} \\leftarrow 0$\n",
    "> 7. $\\hspace{1cm} \\Lambda^* \\leftarrow 0$\n",
    "> 8. $\\hspace{1cm} \\mathcal{X}^* \\leftarrow \\emptyset $\n",
    "> 9. $\\hspace{1cm} b \\leftarrow 1$\n",
    "> 10. $\\hspace{1cm} Done \\leftarrow False$\n",
    "> 11. $\\hspace{1cm} \\textbf{while} \\; Done = False \\; \\textbf{do}$\n",
    "> 12. $\\hspace{2cm} x_{t+1}^*,...,x_b^* \\sim_{w/o \\; repl.} \\mathcal{X} \\backslash \\mathcal{X}^*$\n",
    "> 13. $\\hspace{2cm} \\mathcal{X}^* \\leftarrow \\mathcal{X}^* \\cup \\{ x_{t+1}^*,...,x_b^* \\}$ \n",
    "> 14. $\\hspace{2cm} \\Lambda^* \\leftarrow \\dfrac{1}{b} \\left( t \\Lambda^* + \\sum_{i=t+1}^b \\log \\left[ \\dfrac{p(x_i^*|\\theta')}{p(x_i^*|\\theta)} \\right] \\right)$\n",
    "> 15. $\\hspace{2cm} t \\leftarrow b$\n",
    "> 16. $\\hspace{2cm} c \\leftarrow c_t(\\delta)$ \n",
    "> 17. $\\hspace{2cm} b \\leftarrow n \\land [\\gamma t]$ \n",
    "> 18. $\\hspace{2cm} \\textbf{if} \\; |\\Lambda^* - \\psi(u, \\theta, \\theta')| \\geq c \\; \\textbf{or} \\; t=n$ \n",
    "> 19. $\\hspace{3cm} Done \\leftarrow True$\n",
    "> 20. $\\hspace{1cm} \\textbf{if} \\; \\Lambda^* \\geq \\psi(u, \\theta, \\theta') $\n",
    "> 21. $\\hspace{2cm} \\theta_k \\leftarrow \\theta'$ # __ACCEPT__\n",
    "> 22. $\\hspace{1cm} \\textbf{else} \\; \\theta_k \\leftarrow \\theta \\hspace{2mm}$   # __REJECT__\n",
    "> 23. $\\textbf{return} \\; (\\theta_k)_{k=1,...,N_{iter}} $ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MtKGcszaiZ9o"
   },
   "source": [
    "At each iteration, the algorithm first draws a candidate $\\theta'$ from the proposal distribution and a uniform random number $u$ as in the MH algorithm. $\\psi(u, \\theta, \\theta')$ is also computed. Then, we initialize the variables that will be used to find the correct $\\Lambda^*$ which corresponds to an approximation of the loglikelihood ratio.\n",
    "\n",
    "To compute $\\Lambda^*$, one needs to find the subsample size $b$ such that the right MH acceptance decision is taken with probability at least $1-\\delta$. \n",
    "In order to achieve this, the algorithm uses a while loop for each iteration in which $b$ is increased geometrically. The points of the batch are drawn uniformly without replacement from the dataset. A confidence bound $c_t(\\delta)$ is also computed for a valid concentration inequality such as Hoeffding's or Bernstein's. Finally, the while loop ends until either we have gone over the whole dataset or the confidence condition is satisfied.\n",
    "\n",
    "Afterwards, to decide whether to accept or reject the candidate $\\theta'$, the algorithm compares $\\psi(u, \\theta, \\theta')$ and $\\Lambda^*$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sOFCMfYdid13"
   },
   "source": [
    "#####  <font color='#0000e6'> 3.2.2 Implementation  </font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6gU3TF-ridn0"
   },
   "source": [
    "\n",
    "The confidence sampler is already implemented in Python in the notebook provided by the authors of the paper. So, we did not implement it again. \n",
    "\n",
    "The plot below is a histogram of the batchsizes used by the confidence sampler for a simple example: We fit a one-dimensional normal distribution to $10^5$ i.i.d. points drawn according to a normal distribution $\\mathcal{N}(0,1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_ctfb7Ks86pA"
   },
   "source": [
    "<center>\n",
    "\n",
    "![plot number of iterations in confidence sampler](https://i.imgur.com/BxYpbRM.png[/img])\n",
    "</center>\n",
    "\n",
    "We observe that almost each iteration uses batchsize equals to the total number of individual data points. That is why this confidence sampler is still computationally not efficient, even for this extremely simple example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sEwtFmgHm75-"
   },
   "source": [
    "\n",
    "####  <font color='#0000e6'> 3.3 Improved confidence sampler  </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WAN0HJtHnA0X"
   },
   "source": [
    "##### <font color='#0000e6'> 3.3.1 Improved confidence sampler  </font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tW6dcn2g7LLp"
   },
   "source": [
    "> The authors propose a modified version of the confidence sampler. They use a proxy to replace approximated loglikelihood ratio and the same trick is used: **If we are *confident enough* in our proxy, the acceptance decision should be identical**.\n",
    ">\n",
    "> In an example, we will show that this new algorithm can require less than $O(n)$ likelihood evaluations per iteration.\n",
    ">\n",
    "> The conditions required for this algorithm to work are the following:\n",
    "> For $\\theta, \\theta' \\in \\Theta$\n",
    "> > 1. $\\wp_i (\\theta, \\theta') \\approx l_i(\\theta') - l_i(\\theta)$\n",
    "> > 2. $\\sum_{i=1}^n \\wp_i (\\theta, \\theta')$ is cheap to compute\n",
    "> > 3. $|l_i(\\theta') - l_i(\\theta) - \\wp_i (\\theta, \\theta')|$ can be bounded uniformly for $i \\in [\\![1, n]\\!]$ and the bound is cheap to compute.\n",
    ">\n",
    "> **The third condition is the one that lets us use a concentration inequality and bound the *confidence* in the approximated acceptance ratio.**\n",
    ">\n",
    "> These assumptions are very strong, but this method shows a very promising approach to scaling up MCMC. Let us illustrate this potential with an example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "49kyWRpstyst"
   },
   "source": [
    "#####  <font color='#0000e6'> 3.3.2 Example Proxy: Taylor Expansion  </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sJFcrZrNkNS8"
   },
   "source": [
    "> We assume that around some reference value $\\theta_*$, the log likelihood ratio can be approximated by:\n",
    ">\n",
    "> $$\\hat l_i(\\theta) = l_i(\\theta_*) + g_{i,*}^T(\\theta - \\theta_*) + \\frac{1}{2}(\\theta - \\theta_*)^T H_{i,*}(\\theta - \\theta_*)$$\n",
    ">\n",
    "> where $g_{i,*}$ is the gradient of $l_i$ at $\\theta_*$ and $H_{i,*}$ is the Hessian matrix of $l_i$ at $\\theta_*$.\n",
    ">\n",
    "> We also assume that the third order derivative of $l_i$ is uniformly bounded for all $i$.\n",
    ">\n",
    "> If we now define:\n",
    "> $$\\wp_i(\\theta, \\theta') = \\hat l_i(\\theta') - \\hat l_i(\\theta) \\approx l_i(\\theta') - l_i(\\theta)$$\n",
    ">\n",
    ">Then, this proxy satisfies all the conditions required to be used in the improved confidence sampling algorithm.\n",
    ">\n",
    "> If we have already precomputed:\n",
    "> $$\\hat\\mu = \\frac{1}{n}\\sum_{i = 1}^{n} g_{i,*}$$\n",
    "> $$\\hat S = \\frac{1}{n}\\sum_{i = 1}^{n} H_{i,*}$$\n",
    ">\n",
    "> Then, the following holds:\n",
    "> $$\\frac{1}{n}\\sum_{i=1}^n \\wp_i(\\theta, \\theta') = \\hat\\mu^T(\\theta' - \\theta) + \\frac{1}{2}(\\theta' - \\theta)^T \\hat S(\\theta' + \\theta + 2\\theta_*)$$\n",
    ">\n",
    "> which means that $\\frac{1}{n}\\sum_{i=1}^n \\wp_i(\\theta, \\theta')$ can effectively be computed in $O(1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kKqvsOEpCEqB"
   },
   "source": [
    "As in the last section, the improved confidence sampler with *Taylor expansions* as proxies is also already implemented in the notebook provided. \n",
    "\n",
    "Here we used the same example as in 3.2.2 and we obtain the following histogram with the new method.\n",
    "\n",
    "<center>\n",
    "\n",
    "![plot number of iterations in confidence sampler](https://i.imgur.com/qnTj9ud.png)\n",
    "</center>\n",
    "\n",
    "The resulting Markov chains are the same in both confidence samplers but the the difference is in the number of likelihood evaluations used at each iteration.\n",
    "\n",
    "We note that the batchsizes are in mean drastically smaller compared to the previous confidence sampler. Each iteration uses only a really small quantity of likelihood evaluations to make the decision.\n",
    "\n",
    "**In conclusion**, the improved confidence sampler algorithm yields far better results than all previous methods but it requires strong and specific assumptions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WbiVWsbqpEL7"
   },
   "source": [
    "#####  <font color='#0000e6'> 3.3.2 Application: Logistic Regression  </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SeyatAhfpD2m"
   },
   "source": [
    "> In the logistic regression model, the loglikelihood is given by:\n",
    "> $$l_i(\\theta) = \\phi(y_i x_i^T\\theta)$$\n",
    ">\n",
    "> where $\\phi(z) = -\\mathrm{log}\\left( 1 + e^{-z}\\right)$, $y_i$ and $x_i$ are respectively the labels and the features of $i$th observation.\n",
    ">\n",
    "> Then, we can use the Taylor expansion proxy with:\n",
    "> $$g_{i, *} = \\phi'(y_i x_i^T\\theta_*)y_i x_i$$\n",
    ">\n",
    "> $$H_{i, *} = \\phi''(y_i x_i^T\\theta_*)x_i x_i^T$$\n",
    ">\n",
    "> We also have that $|\\phi'''(z)| =\\left| \\frac{\\mathrm{tanh}(z/2)}{\\mathrm{cosh}^2(z/2)} \\right| \\leq \\frac{1}{4}$, which means that we can uniformly bound $|l_i(\\theta') - l_i(\\theta) - \\wp_i (\\theta, \\theta')|$:\n",
    "> $$|l_i(\\theta') - l_i(\\theta) - \\wp_i (\\theta, \\theta')| \\leq \\frac{1}{24} \\max_{i = 1}^{n} \\|x_i\\|^3 (\\|\\theta' - \\theta_*\\| + \\|\\theta - \\theta_*\\|)$$\n",
    "> \n",
    "> Therefore, this proxy satisfies the conditions required.\n",
    ">\n",
    "> One last detail remains however: **the choice of $\\theta_*$**.\n",
    ">\n",
    "> The authors propose that if one sets $\\theta_*$ to be the state of the Markov chain every $\\alpha$ MH iteration. We will need to do a full pass over the dataset to compute $\\hat\\mu$ and $\\hat S$ but this cost is still overall negligeable.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-2gnYNJfu_XU"
   },
   "source": [
    "<a id='s4'></a>\n",
    "## <font color='#0000e6'>  4. Conclusion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C2-lYu2-nSNq"
   },
   "source": [
    "> Sadly, unlike for gradient descent there are no \"plug-and-play\" approaches to scaling up MCMC methods to tall datasets:\n",
    "> - Divide and conquer approaches are unreliable as they have no theoretical backing for non-Gaussian settings.\n",
    "> - Pseudo-Marginal approximations are difficult to control and as a result are unpredictible *as is*.\n",
    "> - While being theoretically justified, vanilla Confidence Sampling fails to make substantial gains in computation efficiency in practice.\n",
    ">\n",
    "> Thankfully, the scaling of MCMC methods to tall datasets is a very active area of research and novel approaches such as the use of proxies for an improved confidence sampling algorithm show promise and may provide a path forward to solving this problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "StDvQ1pcEMk4"
   },
   "source": [
    "<a id='s5'></a>\n",
    "## <font color='#0000e6'>  5. Code\n",
    "\n",
    "> In this section we present illustrative examples, the user can modify several parameters and visualize the results. Additionally, the codes contain comments for better understanding of the user.<br>\n",
    ">- **Generation of the data:** We generate $n$ i.i.d points drawn according:\n",
    ">  - $X_i \\sim  \\mathcal N( \\mu = 0, \\sigma=1)$ or\n",
    ">  - $X_i \\sim  \\log \\mathcal N( \\mu = 0, \\sigma=1)$<br> <br>\n",
    ">- **Parameters of MH algorithm:**\n",
    ">  - priori distribution: $p(\\theta)=p(\\mu, \\log\\sigma) \\propto 1$\n",
    ">  - proposal distribution $\\theta^*$: isotropic Gaussian random walk, whose stepsize $\\lambda$ is firts set proportional to $\\frac 1 {\\sqrt{n}}$ and then adapted during the first 1 000 iterations so as to reach $50\\%$ acceptance.\n",
    "$$\\theta^* = \\theta + \\lambda Z,$$\n",
    "where $Z\\sim \\mathcal N( 0,I_2)$, $I_2$ is the $2\\times 2$ identity matrix, and $\\lambda > 0$.<br>\n",
    "See: [ON THE EFFICIENCY OF PSEUDO-MARGINAL RANDOM WALK METROPOLIS ALGORITHMS](https://arxiv.org/pdf/1309.7209.pdf)\n",
    ">  - **$N_{ite}:$** number of iterations. <br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wyXMaCYsEL5Z"
   },
   "outputs": [],
   "source": [
    "#@title <font size='4'> Libraries</font> {display-mode: \"form\"}\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "Required packages\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "import scipy.stats as sps\n",
    "import scipy.special as spsp\n",
    "import scipy.misc as spm\n",
    "import scipy.optimize as spo\n",
    "import numpy.linalg as npl\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.cm as cmx\n",
    "import sympy as sym\n",
    "import time\n",
    "import seaborn as sns\n",
    "import seaborn.distributions as snsd\n",
    "import math as math\n",
    "\n",
    "import warnings\n",
    "mpl.style.use('seaborn') #Background\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from IPython.display import HTML\n",
    "from matplotlib import animation, rc\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import HBox, VBox\n",
    "from IPython.display import display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T7HklAUHDAY_"
   },
   "outputs": [],
   "source": [
    "#@title <font size='4'> Metropolis Hastings algorithm code</font> {display-mode: \"form\"}\n",
    "\n",
    "def getLogLhd(x, mu, sigma):\n",
    "    \"\"\"\n",
    "    return an array of Gaussian log likelihoods up to a constant\n",
    "    \"\"\"\n",
    "    return -(x-mu)**2/(2*sigma**2) - np.log(sigma)\n",
    "    \n",
    "def vanillaMH(T):\n",
    "    \"\"\"\n",
    "    perform traditional isotropic random walk Metropolis\n",
    "    T: Number of iterations\n",
    "    \"\"\"\n",
    "    # True value mu, log(sigma^2)\n",
    "    theta = np.array([realMean,np.log(realStd)])\n",
    "    stepsize = .5/np.sqrt(N)\n",
    "    S = np.zeros((T, 2))\n",
    "    acceptance = 0.0\n",
    "    \n",
    "    for i in range(T):\n",
    "        accepted = 0\n",
    "        done = 0\n",
    "        thetaNew = theta\n",
    "        #- Proposal: isotropic random walk (definition)\n",
    "        thetaP = theta + stepsize*npr.randn(2)\n",
    "        u = npr.rand()# Uniform\n",
    "        #- Diference between \\theta' (proposal) and \\theta_{t-1}  (array)\n",
    "        #- to calculate alpha, we have a prior distribution equal to one \n",
    "        lhds = getLogLhd(x, thetaP[0], np.exp(thetaP[1])) - getLogLhd(x, theta[0], np.exp(theta[1]))\n",
    "        #- l(\\theta): average log-likelihood\n",
    "        Lambda = np.mean(lhds)\n",
    "        #- q(\\theta | \\theta')/ q(\\theta' | \\theta) = 1 then log(.) = 0\n",
    "        psi = (1./N)*np.log(u)\n",
    "        if Lambda>psi:#it is equivalent to u < alpha(\\theta,\\theta' )\n",
    "            thetaNew = thetaP\n",
    "            theta = thetaP\n",
    "            accepted = 1\n",
    "            S[i,:] = thetaNew\n",
    "        else:\n",
    "            S[i,:] = theta\n",
    "            \n",
    "        if i<T/10:\n",
    "            # Perform some adaptation of the stepsize in the early iterations\n",
    "            stepsize *= np.exp(1./(i+1)**0.6*(accepted-0.5)) #(?)\n",
    "        #- Historical acceptance rate (each iteration)\n",
    "        acceptance*=i\n",
    "        acceptance+=accepted\n",
    "        acceptance/=(i+1)\n",
    "        if np.mod(i,T/10)==0:\n",
    "          print(\"Iteration %d and  Acceptance percentage %.2f \"%(i,100.*np.round(acceptance,4)))\n",
    "            \n",
    "    return S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145,
     "referenced_widgets": [
      "2ebc8243421d4b8b88724542465eb57e",
      "f88fd9993f0e4826b453bc8d724b416a",
      "528c884ac8d243b2a7b451dc61b4bfc9",
      "d926c4b5c0a548459dce91b89574381a",
      "96ff89c474fe4f98b2bdf952b6967607",
      "07e425b5f36b4f36b2fdc8895c3c17c8",
      "e0ed800defb9414e9221e7aca690843b",
      "c7ece73ba61649babac47cc0491f0edc",
      "0e822e1ddf1548a5b8917b5052d2804a",
      "5956d5faff0d4950b3c1dbe3a68105ce",
      "ba1897bb6e2b45819d196f638cb2b365",
      "4e92041aea2b488dad1eb8871031daba"
     ]
    },
    "colab_type": "code",
    "id": "q7wIEqF75SNt",
    "outputId": "1c3d1929-6851-4017-de32-d9b8686f0213"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ebc8243421d4b8b88724542465eb57e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Tab(children=(VBox(children=(ToggleButtons(description='Distribution:', options=('Normal', 'Log…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#@title <font size='4'> Select the distribution and enter the number of data points :</font> {display-mode: \"form\"}\n",
    "\n",
    "dist = widgets.ToggleButtons(\n",
    "    options=['Normal', 'Log-normal'],\n",
    "    description='Distribution:',\n",
    ")\n",
    "\n",
    "n_p=widgets.IntText(\n",
    "    value=100000,\n",
    "    description='n:',\n",
    "    disabled=False\n",
    ")\n",
    "tab1 = VBox(children=[dist,\n",
    "                      n_p])\n",
    "tab = widgets.Tab(children=[tab1])\n",
    "tab.set_title(0, 'Generation of the data')\n",
    "VBox(children=[tab])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "colab_type": "code",
    "id": "aeGgOdcQDAXU",
    "outputId": "92001797-559f-445d-bed7-db9c1b0a45e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal data simulation\n",
      "Number of points= 100000\n",
      "Mean = 0.0053\n",
      "standard deviation = 0.9983\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAecAAAFZCAYAAACizedRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 576x396 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#@title <font size='4'> Data Generation</font> {display-mode: \"form\"}\n",
    "#- Seed\n",
    "npr.seed(1)\n",
    "# Number of points\n",
    "N = n_p.value\n",
    "#- Distribution \n",
    "dataType = dist.value\n",
    "#- Generation of points\n",
    "if dataType == \"Normal\":\n",
    "    x = npr.randn(N)\n",
    "    print('Normal data simulation')\n",
    "elif dataType == \"Log-normal\":\n",
    "    x = npr.lognormal(0,1,size=N)\n",
    "    print('Log-normal data simulation')\n",
    "# We store the mean and std deviation for later reference, they are also the MAP and MLE estimates in this case.\n",
    "realMean = np.mean(x)\n",
    "realStd = np.std(x)\n",
    "\n",
    "print('Number of points= %d'%N)\n",
    "print(\"Mean =\", np.round(realMean,4))\n",
    "print (\"standard deviation =\", np.round(realStd,4))\n",
    "# Plot of points generates\n",
    "plt.hist(x, 30, normed=True)\n",
    "plt.title('Distribution of the generated data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50,
     "referenced_widgets": [
      "b88be42d9eed4535be0fc6c6c5b94354",
      "78892877728c4e289f423ca7f45933cb",
      "4c71b87605654fd682057921fda157c9"
     ]
    },
    "colab_type": "code",
    "id": "7jY05aZ95w6V",
    "outputId": "1225dc3d-eec0-46f0-d139-0ded494c9e35"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b88be42d9eed4535be0fc6c6c5b94354",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=10000, description='Iterations:')"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#@title <font size='4'> Enter the number of iterations for vanilla MH:</font> {display-mode: \"form\"}\n",
    "n_iter = widgets.IntText(\n",
    "    value = 10000,\n",
    "    description = 'Iterations:',\n",
    "    disabled = False\n",
    ")\n",
    "n_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "_E4tU1MkDAZv",
    "outputId": "d47a7dc6-ae55-4bfd-9cc1-7b16b39795c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 and  Acceptance percentage 0.00 \n",
      "Iteration 1000 and  Acceptance percentage 50.35 \n",
      "Iteration 2000 and  Acceptance percentage 50.72 \n",
      "Iteration 3000 and  Acceptance percentage 50.68 \n",
      "Iteration 4000 and  Acceptance percentage 50.79 \n",
      "Iteration 5000 and  Acceptance percentage 50.43 \n",
      "Iteration 6000 and  Acceptance percentage 50.47 \n",
      "Iteration 7000 and  Acceptance percentage 50.19 \n",
      "Iteration 8000 and  Acceptance percentage 50.18 \n",
      "Iteration 9000 and  Acceptance percentage 49.89 \n"
     ]
    }
   ],
   "source": [
    "#@title <font size='4'> Running vanilla MH algorithm</font> {display-mode: \"form\"}\n",
    "S = vanillaMH(n_iter.value)\n",
    "\n",
    "#plt.figure(figsize=(15,7))\n",
    "#plt.subplot(1,2,1)\n",
    "#plt.plot(S[:,0],label = r\"MH chain of: $\\mu$\")\n",
    "#plt.axhline(0, color='red', label = r\"$\\mu = 0 $\")\n",
    "#plt.ylim([-0.1, 0.1])\n",
    "#plt.xlabel('Iterations')\n",
    "#plt.legend()\n",
    "#plt.subplot(1,2,2)\n",
    "#plt.plot(np.exp(S[:,0]),label = r\"MH chain of: $\\sigma$\")\n",
    "#plt.axhline(1, color='red',label = r\"$\\sigma = 1 $\")\n",
    "#plt.xlabel('Iterations')\n",
    "#plt.legend()\n",
    "#plt.ylim([0.90, 1.1])\n",
    "#plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9hcvn9-o5tOX"
   },
   "outputs": [],
   "source": [
    "#@title <font size='4'> Confidence Sampler implementation</font> {display-mode: \"form\"}\n",
    "\n",
    "## Concentration bounds\n",
    "def ctBernsteinSerfling(N,n,a,b,sigma,delta):\n",
    "    \"\"\"\n",
    "    Bernstein-type bound without replacement, from (Bardenet and Maillard, to appear in Bernoulli)\n",
    "    \"\"\"\n",
    "    l5 = np.log(5/delta)\n",
    "    kappa = 7.0/3+3/np.sqrt(2)\n",
    "    if n<=N/2:\n",
    "        rho = 1-1.0*(n-1)/N\n",
    "    else:\n",
    "        rho = (1-1.0*n/N)*(1+1.0/n)\n",
    "    return sigma*np.sqrt(2*rho*l5/n) + kappa*(b-a)*l5/n\n",
    "\n",
    "def ctHoeffdingSerfling(N,n,a,b,delta):\n",
    "    \"\"\"\n",
    "    Classical Hoeffding-type bound without replacement, from (Serfling, Annals of Stats 1974)\n",
    "    \"\"\"\n",
    "    l2 = np.log(2/delta)\n",
    "    if n<=N/2:\n",
    "        rho = 1-1.0*(n-1)/N\n",
    "    else:\n",
    "        rho = (1-1.0*n/N)*(1+1.0/n)\n",
    "    return (b-a)*np.sqrt(rho*l2/2/n)\n",
    "\n",
    "def ctBernstein(N,n,a,b,sigma,delta):\n",
    "    \"\"\"\n",
    "    Classical Bernstein bound, see e.g. the book by Boucheron, Lugosi, and Massart, 2014.\n",
    "    \"\"\"\n",
    "    l3 = np.log(3/delta)\n",
    "    return sigma*np.sqrt(2*l3/n) + 3*(b-a)*l3/n\n",
    "\n",
    "\n",
    "## Confidence MCMC (Bardenet, Doucet, and Holmes, ICML'14)\n",
    "def confidenceMCMC(T):\n",
    "    \n",
    "    # Initialize\n",
    "    theta = np.array([realMean,np.log(realStd)])\n",
    "    stepsize = .01 \n",
    "    S_B = np.zeros((T,2))\n",
    "    delta = .1\n",
    "    acceptance = 0.0\n",
    "    gamma = 1.5\n",
    "    ns_B = []\n",
    "    \n",
    "    bs = []\n",
    "    \n",
    "    for i in range(T):\n",
    "        \n",
    "        npr.shuffle(x)\n",
    "        accepted = 0\n",
    "        done = 0\n",
    "        thetaNew = theta\n",
    "        thetaP = theta + stepsize*npr.randn(2)\n",
    "        u = npr.rand()\n",
    "        n = N/10\n",
    "        cpt = 0\n",
    "        lhds = getLogLhd(x, thetaP[0], np.exp(thetaP[1])) - getLogLhd(x, theta[0], np.exp(theta[1]))\n",
    "        a = np.min(lhds)\n",
    "        b = np.max(lhds)\n",
    "            \n",
    "        while not done and n<N:\n",
    "        \n",
    "            n = min(N,np.floor(gamma*n))\n",
    "            cpt+=1\n",
    "            deltaP = delta/2/cpt**2\n",
    "            # The following step should be done differently to avoid recomputing previous likelihoods, but for the toy examples we keep it short\n",
    "            lhds = getLogLhd(x[:int(n)], thetaP[0], np.exp(thetaP[1])) - getLogLhd(x[:int(n)], theta[0], np.exp(theta[1])) \n",
    "            Lambda = np.mean(lhds)\n",
    "            sigma = np.std(lhds)\n",
    "            psi = np.log(u)/N\n",
    "            if np.abs(Lambda-psi) > ctBernstein(N,n,a,b,sigma,deltaP):\n",
    "                done = 1\n",
    "        bs.append(n)\n",
    "\n",
    "        if i>1 and ns_B[-1] == 2*N:\n",
    "            ns_B.append(n) # Half of the likelihoods were computed at the previous stage\n",
    "        else:\n",
    "            ns_B.append(2*n) # The algorithm required all likelihoods for theta and theta', next iteration we can reuse half of them\n",
    "        \n",
    "        if Lambda>psi:\n",
    "            # Accept\n",
    "            theta = thetaP\n",
    "            accepted = 1\n",
    "            S_B[i] = thetaP\n",
    "        else:\n",
    "            # Reject\n",
    "            S_B[i] = theta\n",
    "            \n",
    "        if i<T/10:\n",
    "            # Perform some adaptation of the stepsize in the early iterations\n",
    "            stepsize *= np.exp(1./(i+1)**0.6*(accepted-0.5))    \n",
    "            \n",
    "        acceptance*=i\n",
    "        acceptance+=accepted\n",
    "        acceptance/=(i+1)\n",
    "        if np.mod(i,T/10)==0:\n",
    "            # Monitor acceptance and average number of samples used\n",
    "            print(\"Iteration\", i, \"Acceptance\", acceptance, \"Avg. num evals\", np.mean(ns_B), \"sigma/sqrt(n)\", sigma/np.sqrt(n), \"R/n\", (b-a)/n)\n",
    "        \n",
    "    return S_B, ns_B, bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50,
     "referenced_widgets": [
      "932cd58ef2f247ef88170ff6a2eb1536",
      "86405da2620a4e2a8173c81685690cd2",
      "693820394d2b424ab37933f0bea6ebf8"
     ]
    },
    "colab_type": "code",
    "id": "xdQ-Yday79DX",
    "outputId": "a18a83d3-558b-4e7a-e781-d0c62d88d63d"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "932cd58ef2f247ef88170ff6a2eb1536",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=10000, description='Iterations:')"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#@title <font size='4'> Enter the number of iterations for Confidence Sampler:</font> {display-mode: \"form\"}\n",
    "n_iter_conf = widgets.IntText(\n",
    "    value = 10000,\n",
    "    description = 'Iterations:',\n",
    "    disabled = False\n",
    ")\n",
    "n_iter_conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 207
    },
    "colab_type": "code",
    "id": "vEc-W1LF59k8",
    "outputId": "15e8ccdc-d5e3-491d-d566-55424ace386d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 Acceptance 0.0 Avg. num evals 200000.0 sigma/sqrt(n) 4.395225050451293e-05 R/n 1.5390382929998826e-06\n",
      "Iteration 1000 Acceptance 0.4905094905094903 Avg. num evals 148947.92507492506 sigma/sqrt(n) 2.0524631030610058e-05 R/n 5.513361838972485e-07\n",
      "Iteration 2000 Acceptance 0.5237381309345327 Avg. num evals 148457.95452273864 sigma/sqrt(n) 1.221251411125473e-05 R/n 4.979166349999433e-07\n",
      "Iteration 3000 Acceptance 0.5334888370543155 Avg. num evals 148237.14561812728 sigma/sqrt(n) 1.827195489979916e-05 R/n 4.948659768960084e-07\n",
      "Iteration 4000 Acceptance 0.5446138465383661 Avg. num evals 148095.23619095227 sigma/sqrt(n) 3.526295495338081e-06 R/n 1.7891358035839234e-07\n",
      "Iteration 5000 Acceptance 0.5406918616276746 Avg. num evals 148087.6898620276 sigma/sqrt(n) 1.0740827112220162e-05 R/n 3.9614072911498265e-07\n",
      "Iteration 6000 Acceptance 0.5384102649558408 Avg. num evals 148121.50541576403 sigma/sqrt(n) 1.1200128300823346e-05 R/n 3.608274174049231e-07\n",
      "Iteration 7000 Acceptance 0.5380659905727752 Avg. num evals 148088.48178831596 sigma/sqrt(n) 5.56185432900585e-06 R/n 1.707119093347198e-07\n",
      "Iteration 8000 Acceptance 0.5396825396825402 Avg. num evals 147965.3655793026 sigma/sqrt(n) 7.503746148951691e-06 R/n 1.9528925988161916e-07\n",
      "Iteration 9000 Acceptance 0.5364959448950124 Avg. num evals 148026.46328185758 sigma/sqrt(n) 8.007518402064551e-06 R/n 5.138271740664444e-07\n"
     ]
    }
   ],
   "source": [
    "#@title <font size='4'> Running the Confidence Sampler</font> {display-mode: \"form\"}\n",
    "\n",
    "S_B, ns_B, bs = confidenceMCMC(n_iter_conf.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 361
    },
    "colab_type": "code",
    "id": "X9KlUNdX6LGO",
    "outputId": "4cd7a284-a04f-45b3-92f1-139ae7f0b6ca"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAf0AAAFYCAYAAABZHSXVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nETGJ/wc0dAu49dfmvgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 576x396 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#@title <font size='4'> Histogram of the number of likelihood evaluations:</font> {display-mode: \"form\"}\n",
    "plt.hist(bs)\n",
    "\n",
    "labStr = \"mean=\"+str(np.around(1.0*np.mean(bs)/N*100,1))+\"%\"\n",
    "plt.axvline(np.mean(bs), linewidth = 4, color=\"blue\", label=labStr)\n",
    "plt.axvline(N, linewidth = 4, color=\"k\", label=\"n\") \n",
    "labStr = \"median=\"+str(np.around(1.0*np.median(bs)/N*100,1))+\"%\"\n",
    "plt.axvline(np.median(bs), linewidth = 4, color=\"blue\",linestyle='--', label=labStr)\n",
    "plt.legend()\n",
    "plt.xlabel(\"Batchsize\")\n",
    "plt.ylabel(\"Number of iterations\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lzQsAmxF7hCl"
   },
   "outputs": [],
   "source": [
    "#@title <font size='4'> Implementation of the Improved Confidence Sampler with Taylor expansions </font> {display-mode: \"form\"}\n",
    "\n",
    "def combineMeansAndSSQs(N1, mu1, ssq1, N2, mu2, ssq2):\n",
    "    \"\"\"\n",
    "    combine means and sum of squares of two sets\n",
    "    \"\"\"\n",
    "    dd = mu2 - mu1\n",
    "    mu = mu1\n",
    "    ssq = ssq1\n",
    "    N = N1+N2\n",
    "    mu += dd*N2/N\n",
    "    ssq += ssq2\n",
    "    ssq += (dd**2) * N1 * N2 / N\n",
    "    return N, mu, ssq\n",
    "    \n",
    "### Differential functions for proxies, \n",
    "\n",
    "### Define vectorized evaluation of gradient and Hessian\n",
    "myGradientVect = lambda x_float, mu_float, sigma_float:np.array([-(2*mu_float - 2*x_float)/(2*sigma_float**2), -1/sigma_float + (-mu_float + x_float)**2/sigma_float**3]).T\n",
    "myHessianVect = lambda x_float, mu_float, sigma_float:[[-1/sigma_float**2*np.ones(x_float.shape), 2*(mu_float - x_float)/sigma_float**3], [2*(mu_float - x_float)/sigma_float**3, (1 - 3*(mu_float - x_float)**2/sigma_float**2)/sigma_float**2]]\n",
    "\n",
    "# Compute third order derivatives to bound the Taylor remainder. Symbolic differentiation is not really necessary in this simple case, but\n",
    "# it may be useful in later applications\n",
    "def thirdDerivatives():\n",
    "    x, mu, sigma = sym.symbols('x, mu, sigma')\n",
    "    L = []\n",
    "    for i in range(4):\n",
    "        for j in range(4):\n",
    "            if i+j == 3:\n",
    "                args = tuple([-(x-mu)**2/(2*sigma**2) -sym.log(sigma)] + [mu for cpt in range(i)] + [sigma for cpt in range(j)])                \n",
    "                L.append( sym.diff(*args) )\n",
    "    return L\n",
    "    \n",
    "def evalThirdDerivatives(x_float, mu_float, logSigma_float): \n",
    "    tt = thirdDerivatives()\n",
    "    return [tt[i].subs('x',x_float).subs('mu',mu_float).subs('sigma',np.exp(logSigma_float)).evalf() for i in range(4)]\n",
    "\n",
    "# Find the MAP (not really necessary here since the MAP are the mean and std deviation of the data)\n",
    "f = lambda theta: -np.mean(getLogLhd(x, theta[0], np.exp(theta[1])))\n",
    "thetaMAP = spo.minimize(f, np.array([realMean, np.log(realStd)])).x\n",
    "\n",
    "### Confidence MCMC with proxy (Bardenet, Doucet, and Holmes, this submission)\n",
    "def confidenceMCMCWithProxy(T):\n",
    "    \n",
    "    npr.seed(1)\n",
    "    # Initialize\n",
    "    theta = np.array([realMean,np.log(realStd)])\n",
    "    stepsize = .01 \n",
    "    S_B = np.zeros((T,2))\n",
    "    delta = .1\n",
    "    acceptance = 0.0\n",
    "    gamma = 2.\n",
    "    ns_B = []\n",
    "    \n",
    "    # Compute some statistics of the data that will be useful for bounding the error and averaging the proxies    \n",
    "    minx = np.min(x)\n",
    "    maxx = np.max(x)\n",
    "    meanx = np.mean(x)\n",
    "    meanxSquared = np.mean(x**2)\n",
    "    \n",
    "    # Prepare total sum of Taylor proxys\n",
    "    muMAP = thetaMAP[0]\n",
    "    sigmaMAP = np.exp(thetaMAP[1])\n",
    "    meanGradMAP = np.array( [(meanx - muMAP)/sigmaMAP**2, (meanxSquared-2*muMAP*meanx+muMAP**2)/sigmaMAP**3 - 1./sigmaMAP] )\n",
    "    meanHessMAP = np.array( [[-1./sigmaMAP**2, -2*(meanx-muMAP)/sigmaMAP**3], [-2*(meanx-muMAP)/sigmaMAP**3, -3*(meanxSquared-2*muMAP*meanx+muMAP**2)/sigmaMAP**4 + 1/sigmaMAP**2]] )    \n",
    "    \n",
    "    bs = []\n",
    "    \n",
    "    for i in range(T):\n",
    "        \n",
    "        npr.shuffle(x)\n",
    "        accepted = 0\n",
    "        done = 0\n",
    "        thetaNew = theta\n",
    "        thetaP = theta + stepsize*npr.randn(2)\n",
    "        u = npr.rand()\n",
    "        n = 2\n",
    "        t0 = 0\n",
    "        cpt = 0\n",
    "        Lambda = 0\n",
    "        ssq = 0 # Sum of squares\n",
    "        \n",
    "        # Prepare Taylor bounds\n",
    "        xMinusMuMax = np.max(np.abs([1, minx-theta[0], maxx-theta[0], minx-thetaMAP[0], maxx-thetaMAP[0], minx-thetaP[0], maxx-thetaP[0]]))\n",
    "        sigmaMin = np.min(np.exp([theta[1], thetaMAP[1], thetaP[1]]))\n",
    "        R = float(np.max(np.abs(evalThirdDerivatives(xMinusMuMax, 0, np.log(sigmaMin)))))\n",
    "        h = np.array([theta[0]-thetaMAP[0], np.exp(theta[1])-np.exp(thetaMAP[1])])\n",
    "        hP = np.array([thetaP[0]-thetaMAP[0], np.exp(thetaP[1])-np.exp(thetaMAP[1])])\n",
    "        R *= 2*1./6 * max(np.sum(np.abs(h)), np.sum(np.abs(hP)))**3\n",
    "        a = -R\n",
    "        b = R\n",
    "        \n",
    "        # We can already compute the average proxy log likelihood ratio\n",
    "        avgTotalProxy = np.dot(meanGradMAP, hP-h) + .5*np.dot( hP-h, np.dot(meanHessMAP, h+hP) ) \n",
    "        \n",
    "        while not done and n<N:\n",
    "        \n",
    "            n = min(N,np.floor(gamma*n))\n",
    "            cpt+=1\n",
    "            deltaP = delta/2/cpt**2\n",
    "            batch = x[int(t0):int(n)]\n",
    "            lhds = getLogLhd(batch, thetaP[0], np.exp(thetaP[1])) - getLogLhd(batch, theta[0], np.exp(theta[1]))\n",
    "            proxys = np.dot(myGradientVect(batch, muMAP, sigmaMAP), hP-h) + 0.5*np.dot(np.dot(hP-h, myHessianVect(batch,muMAP,sigmaMAP)).T, h+hP)\n",
    "            if np.any(np.abs(lhds-proxys)>R):\n",
    "                # Just a check that our error is correctly bounded\n",
    "                print(\"Taylor remainder is underestimated\")\n",
    "            tmp, Lambda, ssq = combineMeansAndSSQs(t0, Lambda, ssq, n-t0, np.mean(lhds-proxys), (n-t0)*np.var(lhds-proxys))\n",
    "            sigma = np.sqrt(1./n*ssq)\n",
    "            psi = np.log(u)/N\n",
    "            t0 = n\n",
    "            if np.abs(Lambda-psi + avgTotalProxy) > ctBernstein(N,n,a,b,sigma,deltaP):\n",
    "                done = 1\n",
    "        bs.append(n)\n",
    "        \n",
    "        if i>1 and ns_B[-1] == 2*N:\n",
    "            ns_B.append(n) # Half of the likelihoods were computed at the previous stage\n",
    "        else:\n",
    "            ns_B.append(2*n)\n",
    "            \n",
    "        if Lambda+avgTotalProxy>psi:\n",
    "            # Accept\n",
    "            theta = thetaP\n",
    "            accepted = 1\n",
    "            S_B[i] = thetaP\n",
    "        else:\n",
    "            # Reject\n",
    "            S_B[i] = theta\n",
    "            \n",
    "        if i<T/10:\n",
    "            # Perform some adaptation of the stepsize in the early iterations\n",
    "            stepsize *= np.exp(1./(i+1)**0.6*(accepted-0.5))    \n",
    "        \n",
    "        acceptance*=i\n",
    "        acceptance+=accepted\n",
    "        acceptance/=(i+1)\n",
    "        if np.mod(i,T/10)==0:\n",
    "            # Monitor acceptance and average number of samples used\n",
    "            print(\"Iteration\", i, \"Acceptance\", acceptance, \"Avg. num samples\", np.mean(ns_B), \"Dist. to MAP\", np.sum( np.abs(theta-thetaMAP) ), \"sigma/sqrt(n)\", sigma/np.sqrt(n), \"R/n\", R/n)\n",
    "        \n",
    "    return S_B, ns_B, bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 207
    },
    "colab_type": "code",
    "id": "nf9x2-Ri77TE",
    "outputId": "9a385f16-da09-4464-966c-fb143cf41178"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 Acceptance 0.0 Avg. num samples 1024.0 Dist. to MAP 0.0 sigma/sqrt(n) 3.1026756854543736e-09 R/n 6.472474098772885e-08\n",
      "Iteration 1000 Acceptance 0.49850149850149844 Avg. num samples 859.7082917082917 Dist. to MAP 0.0002493180558642231 sigma/sqrt(n) 4.594563220871523e-11 R/n 5.055738236017883e-08\n",
      "Iteration 2000 Acceptance 0.5072463768115952 Avg. num samples 954.5347326336831 Dist. to MAP 0.0061598892790331004 sigma/sqrt(n) 5.239127046418569e-09 R/n 3.4652091675663985e-08\n",
      "Iteration 3000 Acceptance 0.501166277907365 Avg. num samples 941.1449516827724 Dist. to MAP 0.010949986805270207 sigma/sqrt(n) 4.188643171210579e-08 R/n 1.8371592178874226e-07\n",
      "Iteration 4000 Acceptance 0.49337665583604184 Avg. num samples 960.5618595351162 Dist. to MAP 0.0029151686946896163 sigma/sqrt(n) 2.1925098484149743e-08 R/n 1.9431378819589596e-07\n",
      "Iteration 5000 Acceptance 0.49610077984403195 Avg. num samples 1092.4599080183964 Dist. to MAP 0.0010866662283205996 sigma/sqrt(n) 2.6516201557106773e-08 R/n 2.1103632341548628e-07\n",
      "Iteration 6000 Acceptance 0.4955840693217803 Avg. num samples 1144.0079986668889 Dist. to MAP 0.005822140513723733 sigma/sqrt(n) 5.408024042710274e-08 R/n 5.367141233132221e-07\n",
      "Iteration 7000 Acceptance 0.495072132552493 Avg. num samples 1145.1209827167547 Dist. to MAP 0.005379563166217382 sigma/sqrt(n) 4.9496544063918823e-11 R/n 1.827944262401288e-07\n",
      "Iteration 8000 Acceptance 0.4953130858642674 Avg. num samples 1119.8710161229847 Dist. to MAP 0.0037834568928888806 sigma/sqrt(n) 9.3969092335661e-09 R/n 1.357455396500425e-07\n",
      "Iteration 9000 Acceptance 0.49616709254527314 Avg. num samples 1138.8396844795022 Dist. to MAP 0.004839996649832641 sigma/sqrt(n) 1.2120683716210768e-09 R/n 7.038167027504496e-08\n"
     ]
    }
   ],
   "source": [
    "#@title <font size='4'> Running the Improved Confidence Sampler</font> {display-mode: \"form\"}\n",
    "\n",
    "S_BP, ns_BP, bs2 = confidenceMCMCWithProxy(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 361
    },
    "colab_type": "code",
    "id": "EUqcELVO8OY9",
    "outputId": "601d05cd-03b9-4840-a00e-d25aa7b1ba8b"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAf0AAAFYCAYAAABZHSXVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nXefhbgZYiZ4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 576x396 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#@title <font size='4'> Histogram of the number of likelihood evaluations:</font> {display-mode: \"form\"}\n",
    "plt.hist(bs2)\n",
    "\n",
    "labStr = \"mean=\"+str(np.around(1.0*np.mean(bs2)/N*100,1))+\"%\"\n",
    "plt.axvline(np.mean(bs2), linewidth = 4, color=\"blue\", label=labStr)\n",
    "plt.axvline(N, linewidth = 4, color=\"k\", label=\"n\") \n",
    "labStr = \"median=\"+str(np.around(1.0*np.median(bs2)/N*100,1))+\"%\"\n",
    "plt.axvline(np.median(bs2), linewidth = 4, color=\"blue\",linestyle='--', label=labStr)\n",
    "plt.legend()\n",
    "plt.xlabel(\"Batchsize\")\n",
    "plt.ylabel(\"Number of iterations\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2yPI-R7GDSuz"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "colab": {
   "collapsed_sections": [],
   "name": "Projet_Bayesian.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "07e425b5f36b4f36b2fdc8895c3c17c8": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0e822e1ddf1548a5b8917b5052d2804a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ToggleButtonsStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ToggleButtonsStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "button_width": "",
      "description_width": "",
      "font_weight": ""
     }
    },
    "2ebc8243421d4b8b88724542465eb57e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_528c884ac8d243b2a7b451dc61b4bfc9"
      ],
      "layout": "IPY_MODEL_f88fd9993f0e4826b453bc8d724b416a"
     }
    },
    "4c71b87605654fd682057921fda157c9": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4e92041aea2b488dad1eb8871031daba": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "528c884ac8d243b2a7b451dc61b4bfc9": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "TabModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "TabModel",
      "_titles": {
       "0": "Generation of the data"
      },
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "TabView",
      "box_style": "",
      "children": [
       "IPY_MODEL_96ff89c474fe4f98b2bdf952b6967607"
      ],
      "layout": "IPY_MODEL_d926c4b5c0a548459dce91b89574381a",
      "selected_index": 0
     }
    },
    "5956d5faff0d4950b3c1dbe3a68105ce": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "693820394d2b424ab37933f0bea6ebf8": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "78892877728c4e289f423ca7f45933cb": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "86405da2620a4e2a8173c81685690cd2": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "932cd58ef2f247ef88170ff6a2eb1536": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "IntTextModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "IntTextModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "IntTextView",
      "continuous_update": false,
      "description": "Iterations:",
      "description_tooltip": null,
      "disabled": false,
      "layout": "IPY_MODEL_693820394d2b424ab37933f0bea6ebf8",
      "step": 1,
      "style": "IPY_MODEL_86405da2620a4e2a8173c81685690cd2",
      "value": 10000
     }
    },
    "96ff89c474fe4f98b2bdf952b6967607": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_e0ed800defb9414e9221e7aca690843b",
       "IPY_MODEL_c7ece73ba61649babac47cc0491f0edc"
      ],
      "layout": "IPY_MODEL_07e425b5f36b4f36b2fdc8895c3c17c8"
     }
    },
    "b88be42d9eed4535be0fc6c6c5b94354": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "IntTextModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "IntTextModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "IntTextView",
      "continuous_update": false,
      "description": "Iterations:",
      "description_tooltip": null,
      "disabled": false,
      "layout": "IPY_MODEL_4c71b87605654fd682057921fda157c9",
      "step": 1,
      "style": "IPY_MODEL_78892877728c4e289f423ca7f45933cb",
      "value": 10000
     }
    },
    "ba1897bb6e2b45819d196f638cb2b365": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c7ece73ba61649babac47cc0491f0edc": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "IntTextModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "IntTextModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "IntTextView",
      "continuous_update": false,
      "description": "n:",
      "description_tooltip": null,
      "disabled": false,
      "layout": "IPY_MODEL_4e92041aea2b488dad1eb8871031daba",
      "step": 1,
      "style": "IPY_MODEL_ba1897bb6e2b45819d196f638cb2b365",
      "value": 100000
     }
    },
    "d926c4b5c0a548459dce91b89574381a": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e0ed800defb9414e9221e7aca690843b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ToggleButtonsModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ToggleButtonsModel",
      "_options_labels": [
       "Normal",
       "Log-normal"
      ],
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ToggleButtonsView",
      "button_style": "",
      "description": "Distribution:",
      "description_tooltip": null,
      "disabled": false,
      "icons": [],
      "index": 0,
      "layout": "IPY_MODEL_5956d5faff0d4950b3c1dbe3a68105ce",
      "style": "IPY_MODEL_0e822e1ddf1548a5b8917b5052d2804a",
      "tooltips": []
     }
    },
    "f88fd9993f0e4826b453bc8d724b416a": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
